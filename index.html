<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Eunchae Lim - Personal Page</title>
    <style>
        @font-face {
            font-family: 'CustomFont';
            src: url('fonts/CustomFont.otf') format('opentype');
            font-weight: normal;
            font-style: normal;
        }

        body {
            font-family: Arial, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #e6f3ff;
        }
        
        h1, h3 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db; /* h1Îßå Î∞ëÏ§Ñ Ïú†ÏßÄ */
            font-size: 2.5em;
            margin-bottom: 2px;
        }
        
        h2 {
            color: #2c3e50;
            font-size: 2.0em;
            margin-top: 0px;
        }

        h3 {
            font-size: 2em;
            margin-top: 40px;
        }
        .email {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 20px;
        }
        
        /* ÌÉÄÏù¥Ìïë Ïï†ÎãàÎ©îÏù¥ÏÖòÏùÑ ÏúÑÌïú ÌÖçÏä§Ìä∏ Ïä§ÌÉÄÏùº */
        .typing {
            white-space: nowrap;
            overflow: hidden;
            display: inline-block;
            vertical-align: bottom;
        }
        
        /* Ïª§ÏÑú Ïä§ÌÉÄÏùº */
        .cursor {
            display: inline-block;
            background-color: transparent;
            animation: blink 0.7s steps(1) infinite;
            font-size: 1.5em;
            vertical-align: bottom;
        }
        
        /* ÍπúÎ∞ïÏù¥Îäî Ïª§ÏÑú Ìö®Í≥º */
        @keyframes blink {
            50% { opacity: 0; }
        }
        
        .profile-container {
            display: flex;
            align-items: flex-start;
            gap: 30px;
            flex-wrap: wrap;
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .profile-content {
            flex: 1;
            min-width: 300px;
        }
        .profile-image {
            width: 200px;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        .links {
            margin-top: 20px;
        }
        .links a {
            margin-right: 15px;
            color: #3498db;
            text-decoration: none;
            font-weight: bold;
            transition: color 0.3s ease;
        }
        .links a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .publication-title a {
            text-decoration: none; /* ÎßÅÌÅ¨Ïùò Î∞ëÏ§Ñ Ï†úÍ±∞ */
            color: #2c3e50; /* ÎßÅÌÅ¨ ÏÉâÏÉÅ ÏÑ§Ï†ï (ÌïÑÏöîÏóê Îî∞Îùº Ï°∞Ï†ï) */
        }

        .publication-title a:hover {
            text-decoration: underline; /* ÎßàÏö∞Ïä§ Ïò§Î≤Ñ Ïãú Î∞ëÏ§Ñ ÌëúÏãú (ÏÑ†ÌÉùÏÇ¨Ìï≠) */
        }
        .publication {
            display: flex;
            gap: 20px;
            margin-bottom: 30px;
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .publication-image {
            width: 200px;
            height: auto;
            border-radius: 5px;
        }
        .publication-content {
            flex: 1;
        }
        .publication-title {
            font-size: 1.2em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 10px;
        }
        .publication-authors {
            font-style: italic;
            margin-bottom: 10px;
        }
        .publication-venue {
            font-weight: bold;
            color: #7f8c8d;
            margin-bottom: 10px;
        }
         .publication-links a {
            margin-right: 10px;
            color: #3498db;
            text-decoration: none;
        }
        .publication-links a:hover {
            text-decoration: underline;
        }
        .author-highlight {
            font-weight: bold;
            text-decoration: underline;
        }
        .journal-highlight {
            text-decoration: underline;
            background-color: lightblue;
            color: black;
            border-radius: 10px;
            padding: 5px;
        }
        .footnote {
            font-size: 0.8em;
            margin-top: 30px;
            color: #7f8c8d;
            text-align: right;
        }
        @media (max-width: 768px) {
            .profile-container, .publication {
                flex-direction: column;
                align-items: center;
            }
            .profile-image, .publication-image {
                margin-bottom: 20px;
            }
        }
    </style>
</head>
<body>
    <h1>Eunchae Lim</h1>
    <h2>üíª I'm <span class="typing" id="dynamic-text"></span><span class="cursor">|</span></h2>
    <p class="email">218354@jnu.ac.kr, enechae78@gmail.com</p>
    <div class="profile-container">
        <img src="images/profile.jpg" alt="Eunchae Lim" class="profile-image">
        <div class="profile-content">
            <p>Hello! I am a PhD student at the Department of Artificial Intelligence Convergence at <a href="https://global.jnu.ac.kr/jnumain_en.aspx">Chonnam National University</a>. I work under the supervision of <a href="http://sclab.jnu.ac.kr/index.php/about-us/">Hyung-Jeong Yang</a>. My research focuses on how AI can learn the aspects of human-human interaction when interacting with people. In addition, I am interested in researching and developing a model that recognizes human emotions through multi-modalities. Emotion recognition is basic research that can be used to detect depression, improve social interactions, and predict empathy.</p>
            
            <div class="links">
                <a href="https://scholar.google.com/citations?user=8epaYTwAAAAJ&hl=ko">Google Scholar</a><a href="https://drive.google.com/file/d/1Tq6ESek74kzgelgKDSuFt2oSargGWvUF/view?usp=sharing">CV</a><a href="https://living-slicer-ba1.notion.site/Eunchae-Lim-s-Portfolio-b1e53866681e4d2cae95df397f97fd38">Portfolio</a>
            </div>
        </div>
    </div>
    
    <h3>Publications</h3> <p class="footnote">* indicates equal contribution.</p>

    <div class="publication">
        <img src="images/depression.png" alt="Publication 7" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://dl.acm.org/doi/10.1145/3689062.3689085">A lightweight approach based on cross-modality for depression detection</a></div>
            <div class="publication-authors"><span class="author-highlight">Eunchae Lim</span>, Min Jhon, Ju-Wan Kim, Soo-Hyung Kim, Seungwon Kim, and Hyung-Jeong Yang </div>
            <div class="publication-venue">Computers in Biology and Medicine <span class="journal-highlight">(IF:7, JCR 2.3%)</span>, 2025</div>
            <div class="publication-links">
                <a href="https://doi.org/10.1016/j.compbiomed.2024.109618">paper</a><a href="https://github.com/Sclab-Projects-2023/depression-detection">code</a>
            </div>
        </div>
    </div>
    
    <div class="publication">
        <img src="images/muse.png" alt="Publication 6" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://dl.acm.org/doi/10.1145/3689062.3689085">Modality Weights Based Fusion Model for Social Perception Prediction in Video, Audio, and Text</a></div>
            <div class="publication-authors"><span class="author-highlight">Eunchae Lim*</span>, Hyeon-Ji Yang*, Hyung-Jeong Yang, Soo-Hyung Kim, Seungwon Kim, Ji-Eun Shin, and Aera Kim</div>
            <div class="publication-venue">MuSe'24 (ACMMM24 Workshop), 2024</div>
            <div class="publication-links">
                <a href="https://dl.acm.org/doi/pdf/10.1145/3689062.3689085">paper</a>
            </div>
        </div>
    </div>
    
    <div class="publication">
        <img src="images/gce.jpg" alt="Publication 1" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://www.mdpi.com/2076-3417/14/15/6742">GCE: An Audio-Visual Dataset for Group Cohesion and Emotion Analysis</a></div>
            <div class="publication-authors"><span class="author-highlight">Eunchae Lim*</span>, Ngoc-Huynh Ho*, Sudarshan Pant, Young-Shin Kang, Seong-Eun Jeon, Seungwon Kim, Soo-Hyung Kim, and Hyung-Jeong Yang</div>
            <div class="publication-venue">Applied Sciences, 2024</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1fjBx8OMXBcuMh6UPsJZgmyzD-ntNxAX_/view">paper</a><a href="https://sites.google.com/view/gcedataset">dataset</a>
            </div>
        </div>
    </div>

    <div class="publication">
        <img src="images/phymer.jpg" alt="Publication 2" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://ieeexplore.ieee.org/document/10265252">PhyMER: Physiological Dataset for Multimodal Emotion Recognition With Personality as a Context</a></div>
            <div class="publication-authors">Sudarshan Pant, Hyung-Jeong Yang, <span class="author-highlight">Eunchae Lim</span>, Soo-Hyung Kim, Seok-Bong Yoo</div>
            <div class="publication-venue">IEEE Access, 2023</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1pKrFBYybWX2dUsz6SZOGvxfq4HPh22Kr/view">paper</a><a href="https://sites.google.com/view/phymer-dataset/">dataset</a>
            </div>
        </div>
    </div> 

    <div class="publication">
        <img src="images/kdemor.jpg" alt="Publication 3" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://ieeexplore.ieee.org/document/10265252">Korean Drama Scene Transcript Dataset for Emotion Recognition in Conversations</a></div>
            <div class="publication-authors">Sudarshan Pant, <span class="author-highlight">Eunchae Lim</span>, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim, Young-Shin Kang, Hyerim Jang</div>
            <div class="publication-venue">IEEE Access, 2022</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1tSGdNC_efkkVy030A4bcswpjG83PlFlz/view">paper</a><a href="https://sites.google.com/view/kd-emor/">dataset</a>
            </div>
        </div>
    </div> 

    <div class="publication">
        <img src="images/meta.jpg" alt="Publication 4" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://link.springer.com/chapter/10.1007/978-3-031-38165-2_81">Metadata Model Construction and Annotation Framework to Build Product Data Repository for Cloud Manufacturing</a></div>
            <div class="publication-authors"><span class="author-highlight">Eunchae Lim</span>, Changyeong Kim, Zeyue Lin, Yinfeng Shen, Shengyu Liu, Kyoung-Yun Kim, Hyung-Jeong Yang</div>
            <div class="publication-venue">International Conference on Flexible Automation and Intelligent Manufacturing (FAIM 2023)</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1V3eWC-HeDGuxTUSQXIFE5jXjOdJ0Zwzh/view">paper</a><p class="footnote">Joint Research with Wayne State Univ (USA)</p>
            </div>
        </div>
    </div> 

    <div class="publication">
        <img src="images/abaw.jpg" alt="Publication 5" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://ieeexplore.ieee.org/document/9857450">An Attention-Based Method for Multi-Label Facial Action Unit Detection</a></div>
            <div class="publication-authors">Duy Le Hoai, <span class="author-highlight">Eunchae Lim</span>, Eunbin Choi, Sieun Kim, Sudarshan Pant, Guee-Sang Lee, Soo-Huyng Kim, Hyung-Jeong Yang</div>
            <div class="publication-venue">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1SWN13Scurif1-IwjAK79Kne2TXLbEXky/view">paper</a>
            </div>
        </div>
    </div> 
    <script src="script.js"></script>
</body>
</html>
