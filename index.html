<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Eunchae Lim - Personal Page</title>
    <style>
        @font-face {
            font-family: 'CustomFont';
            src: url('fonts/CustomFont.otf') format('opentype');
            font-weight: normal;
            font-style: normal;
        }

        body {
            font-family: Arial, sans-serif;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background-color: #e6f3ff;
        }
        
        h1, h3 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db; /* h1Îßå Î∞ëÏ§Ñ Ïú†ÏßÄ */
            font-size: 2.5em;
            margin-bottom: 2px;
        }
        
        h2 {
            color: #2c3e50;
            font-size: 2.0em;
            margin-top: 0px;
        }

        h3 {
            font-size: 2em;
            margin-top: 40px;
        }
        .email {
            font-style: italic;
            color: #7f8c8d;
            margin-bottom: 20px;
        }
        
        /* ÌÉÄÏù¥Ìïë Ïï†ÎãàÎ©îÏù¥ÏÖòÏùÑ ÏúÑÌïú ÌÖçÏä§Ìä∏ Ïä§ÌÉÄÏùº */
        .typing {
            white-space: nowrap;
            overflow: hidden;
            display: inline-block;
            vertical-align: bottom;
        }
        
        /* Ïª§ÏÑú Ïä§ÌÉÄÏùº */
        .cursor {
            display: inline-block;
            background-color: transparent;
            animation: blink 0.7s steps(1) infinite;
            font-size: 1.5em;
            vertical-align: bottom;
        }
        
        /* ÍπúÎ∞ïÏù¥Îäî Ïª§ÏÑú Ìö®Í≥º */
        @keyframes blink {
            50% { opacity: 0; }
        }
        
        .profile-container {
            display: flex;
            align-items: flex-start;
            gap: 30px;
            flex-wrap: wrap;
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .profile-content {
            flex: 1;
            min-width: 300px;
        }
        .profile-image {
            width: 200px;
            height: auto;
            border-radius: 10px;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        .links {
            margin-top: 20px;
        }
        .links a {
            margin-right: 15px;
            color: #3498db;
            text-decoration: none;
            font-weight: bold;
            transition: color 0.3s ease;
        }
        .links a:hover {
            color: #2980b9;
            text-decoration: underline;
        }
        .publication-title a {
            text-decoration: none; /* ÎßÅÌÅ¨Ïùò Î∞ëÏ§Ñ Ï†úÍ±∞ */
            color: #2c3e50; /* ÎßÅÌÅ¨ ÏÉâÏÉÅ ÏÑ§Ï†ï (ÌïÑÏöîÏóê Îî∞Îùº Ï°∞Ï†ï) */
        }

        .publication-title a:hover {
            text-decoration: underline; /* ÎßàÏö∞Ïä§ Ïò§Î≤Ñ Ïãú Î∞ëÏ§Ñ ÌëúÏãú (ÏÑ†ÌÉùÏÇ¨Ìï≠) */
        }
        .publication {
            display: flex;
            gap: 20px;
            margin-bottom: 30px;
            background-color: white;
            padding: 20px;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .publication-image {
            width: 200px;
            height: auto;
            border-radius: 5px;
        }
        .publication-content {
            flex: 1;
        }
        .publication-title {
            font-size: 1.2em;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 10px;
        }
        .publication-authors {
            font-style: italic;
            margin-bottom: 10px;
        }
        .publication-venue {
            font-weight: bold;
            color: #7f8c8d;
            margin-bottom: 10px;
        }
         .publication-links a {
            margin-right: 10px;
            color: #3498db;
            text-decoration: none;
        }
        .publication-links a:hover {
            text-decoration: underline;
        }
        .author-highlight {
            font-weight: bold;
            text-decoration: underline;
        }
        .journal-highlight {
            color: black;
            padding: 5px;
        }
        .footnote {
            font-size: 0.8em;
            margin-top: 30px;
            color: #7f8c8d;
            text-align: right;
        }
        @media (max-width: 768px) {
            .profile-container, .publication {
                flex-direction: column;
                align-items: center;
            }
            .profile-image, .publication-image {
                margin-bottom: 20px;
            }
        }
    </style>
</head>
<body>
    <h1>Eunchae Lim</h1>
    <h2>üíª I'm <span class="typing" id="dynamic-text"></span><span class="cursor">|</span></h2>
    <p class="email">218354@jnu.ac.kr, enechae78@gmail.com</p>
    <div class="profile-container">
        <img src="images/profile.jpg" alt="Eunchae Lim" class="profile-image">
        <div class="profile-content">
            <p>Hello! I am a PhD student at the Department of Artificial Intelligence Convergence at <a href="https://global.jnu.ac.kr/jnumain_en.aspx">Chonnam National University</a>. I work under the supervision of <a href="http://sclab.jnu.ac.kr/index.php/about-us/">Hyung-Jeong Yang</a>. My research focuses on how AI can learn the aspects of human-human interaction when interacting with people. In addition, I am interested in researching and developing a model that recognizes human emotions through multi-modalities. Emotion recognition is basic research that can be used to detect depression, improve social interactions, and predict empathy.</p>
            
            <div class="links">
                <a href="https://scholar.google.com/citations?user=8epaYTwAAAAJ&hl=ko">Google Scholar</a><a href="https://drive.google.com/file/d/1Tq6ESek74kzgelgKDSuFt2oSargGWvUF/view?usp=sharing">CV</a><a href="https://living-slicer-ba1.notion.site/Eunchae-Lim-s-Portfolio-b1e53866681e4d2cae95df397f97fd38">Portfolio</a>
            </div>
        </div>
    </div>
    
    <h3>Publications</h3> <p class="footnote">* indicates equal contribution.</p>

    <div class="publication">
        <img src="images/depression.png" alt="Publication 7" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://doi.org/10.1016/j.compbiomed.2024.109618">A lightweight approach based on cross-modality for depression detection</a></div>
            <div class="publication-authors"><span class="author-highlight">Eunchae Lim</span>, Min Jhon, Ju-Wan Kim, Soo-Hyung Kim, Seungwon Kim, and Hyung-Jeong Yang </div>
            <div class="publication-venue">Computers in Biology and Medicine <span class="journal-highlight">(IF:7, JCR 2.3%)</span>, 2025</div>
            <div class="publication-links">
                <a href="https://pdf.sciencedirectassets.com/271150/1-s2.0-S0010482524X00189/1-s2.0-S0010482524017037/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEPz%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIC3nDNvS7VxWpJc5h6MgJXcZhhHaAxuGhEfSvY1FP%2FAwAiEA4zPwOLmuN%2FCeWcPZLtaZU4OQVpumZ5ul7YNUFAg26OYqswUIFRAFGgwwNTkwMDM1NDY4NjUiDItojks9NIk%2FPv%2BCwyqQBbgCxu7KG9i%2FOTgDVA08awv5nhRGAkUk8mLXYTOgitYnhYPmuvP8VxT5siKi%2B9sF%2FajG9mkzO42hq860Dn3jC%2Fi7pJIBQnUB%2BBhCj8jpdArG3QPgMXh2r9YgCIaMpugPuCdCHtFpy6hJmQe021GVKY%2Fzjj3f33lFMEvJ%2Fjg%2FpyIBo84sa6vJ2qNANXrYqZYDDS2H4DCCB54RQ4AHbmiQOrHXAznMUpM2rYf9ZgA%2FbNC38e4BreFslhxFFsl%2Be5Wq0VqMhYafkB59OfqXuP6AKYau6xy0dht8buZE06szjSO2JOuMHQhjXfzMoLDT8SLANGHO4S6qFAzKoHjelY5pWXhV4cHw0JR8yXpiea8SBE8JYJHtakG5ceUBhpSZlMGo70eob2Du38BGho2cOCqwxYHWwDqn8JYlgc%2FwYlbo15Jz2%2B4uIyYHspz5kNKxjOv4pvh%2FQth47V7u0D3zivLuvL0M%2BAzjotSawP1BW0YzaHrxUXDVoBEJxzBumE6J8QhE7tKg4bTvz%2B6pNc3ZhOayrIMXWrLegG%2FAlZ3VRJwmJyKFc7qwd8xqMpK%2FF7W%2BPTV8mxBWyAUSsdQS%2B%2BBX8ShYyiHSDGNMqu%2BXvAXEp%2FaonCI3plBuj11s5XQEH9rEuQg2SKzGl6PFAL4h96OeP6meQrKrAoFrpjBc0TsaqUDhyYT3FVvzUsS7Z9NkCdEVsGT7rWnBZIn7BWkhOqLJUma8alih7Zvfto8nO%2Fpg4rT%2FPZBk8wtWTAX16HiPyTemA%2FdQaOONsY5ep9QLNGguqtG0LyV3zIZRq1e9jtgbdvLrGZJXdkfiyiUK6hbdcKCCzatQNEaMjIjs%2BD37FPS05lA3xliEYMzu9MHQGUgZnYRe580hMMfagr0GOrEBTYbdC8lczR2BmmArtcdzJxspzgm01dy7YIxWLn7Mn%2B66Is2NO9H3YeitdSsi7VnMgN7RKWJbZ31u4CxgtV56RE2X7gmP0rB5P%2Blz9ozPe%2B732QVCbi7Fk8sjLbHDhtk3S5ty0pKaiQZYqZaVt2HlWUYiL0E4QGc7t2Onr2e00br7bdYj86PrrFp7RbcbTl332CjUuJ9w5lwPUFjyqU2oENHYBOsjviUGZ9jP5%2BdR1Srh&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250203T133604Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYXICL5HM5%2F20250203%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=56e4e58d420286fc033b0a01b146d99f37488e21f73aca358578bf5aa6eb49cc&hash=2f736e21ed17b00edeaac035722ae7ca27066322b5d0ee1646df5b86b5df7c04&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0010482524017037&tid=spdf-d7c98338-6ee3-4e55-b5e8-d331943493d8&sid=a68549ef26118346b73960d801132d755e9agxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0d1c5c5301550d030157&rr=90c2cec70816ea23&cc=kr">paper</a><a href="https://github.com/Sclab-Projects-2023/depression-detection">code</a>
            </div>
        </div>
    </div>
    
    <div class="publication">
        <img src="images/muse.png" alt="Publication 6" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://dl.acm.org/doi/10.1145/3689062.3689085">Modality Weights Based Fusion Model for Social Perception Prediction in Video, Audio, and Text</a></div>
            <div class="publication-authors"><span class="author-highlight">Eunchae Lim*</span>, Hyeon-Ji Yang*, Hyung-Jeong Yang, Soo-Hyung Kim, Seungwon Kim, Ji-Eun Shin, and Aera Kim</div>
            <div class="publication-venue">MuSe'24 (ACMMM24 Workshop), 2024</div>
            <div class="publication-links">
                <a href="https://sclab.jnu.ac.kr:5040/pdf-viewer?file_id=09380e87a92b345091d7a616b947ce1b">paper</a>
            </div>
        </div>
    </div>
    
    <div class="publication">
        <img src="images/gce.jpg" alt="Publication 1" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://www.mdpi.com/2076-3417/14/15/6742">GCE: An Audio-Visual Dataset for Group Cohesion and Emotion Analysis</a></div>
            <div class="publication-authors"><span class="author-highlight">Eunchae Lim*</span>, Ngoc-Huynh Ho*, Sudarshan Pant, Young-Shin Kang, Seong-Eun Jeon, Seungwon Kim, Soo-Hyung Kim, and Hyung-Jeong Yang</div>
            <div class="publication-venue">Applied Sciences, 2024</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1fjBx8OMXBcuMh6UPsJZgmyzD-ntNxAX_/view">paper</a><a href="https://sites.google.com/view/gcedataset">dataset</a>
            </div>
        </div>
    </div>

    <div class="publication">
        <img src="images/phymer.jpg" alt="Publication 2" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://ieeexplore.ieee.org/document/10265252">PhyMER: Physiological Dataset for Multimodal Emotion Recognition With Personality as a Context</a></div>
            <div class="publication-authors">Sudarshan Pant, Hyung-Jeong Yang, <span class="author-highlight">Eunchae Lim</span>, Soo-Hyung Kim, Seok-Bong Yoo</div>
            <div class="publication-venue">IEEE Access, 2023</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1pKrFBYybWX2dUsz6SZOGvxfq4HPh22Kr/view">paper</a><a href="https://sites.google.com/view/phymer-dataset/">dataset</a>
            </div>
        </div>
    </div> 

    <div class="publication">
        <img src="images/kdemor.jpg" alt="Publication 3" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://ieeexplore.ieee.org/document/10265252">Korean Drama Scene Transcript Dataset for Emotion Recognition in Conversations</a></div>
            <div class="publication-authors">Sudarshan Pant, <span class="author-highlight">Eunchae Lim</span>, Hyung-Jeong Yang, Guee-Sang Lee, Soo-Hyung Kim, Young-Shin Kang, Hyerim Jang</div>
            <div class="publication-venue">IEEE Access, 2022</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1tSGdNC_efkkVy030A4bcswpjG83PlFlz/view">paper</a><a href="https://sites.google.com/view/kd-emor/">dataset</a>
            </div>
        </div>
    </div> 

    <div class="publication">
        <img src="images/meta.jpg" alt="Publication 4" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://link.springer.com/chapter/10.1007/978-3-031-38165-2_81">Metadata Model Construction and Annotation Framework to Build Product Data Repository for Cloud Manufacturing</a></div>
            <div class="publication-authors"><span class="author-highlight">Eunchae Lim</span>, Changyeong Kim, Zeyue Lin, Yinfeng Shen, Shengyu Liu, Kyoung-Yun Kim, Hyung-Jeong Yang</div>
            <div class="publication-venue">International Conference on Flexible Automation and Intelligent Manufacturing (FAIM 2023)</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1V3eWC-HeDGuxTUSQXIFE5jXjOdJ0Zwzh/view">paper</a><p class="footnote">Joint Research with Wayne State Univ (USA)</p>
            </div>
        </div>
    </div> 

    <div class="publication">
        <img src="images/abaw.jpg" alt="Publication 5" class="publication-image">
        <div class="publication-content">
            <div class="publication-title"><a href="https://ieeexplore.ieee.org/document/9857450">An Attention-Based Method for Multi-Label Facial Action Unit Detection</a></div>
            <div class="publication-authors">Duy Le Hoai, <span class="author-highlight">Eunchae Lim</span>, Eunbin Choi, Sieun Kim, Sudarshan Pant, Guee-Sang Lee, Soo-Huyng Kim, Hyung-Jeong Yang</div>
            <div class="publication-venue">2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</div>
            <div class="publication-links">
                <a href="https://drive.google.com/file/d/1SWN13Scurif1-IwjAK79Kne2TXLbEXky/view">paper</a>
            </div>
        </div>
    </div> 
    <script src="script.js"></script>
</body>
</html>
